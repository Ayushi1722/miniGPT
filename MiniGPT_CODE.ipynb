{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WgzhnLjpQ8iu"
      },
      "outputs": [],
      "source": [
        "import math, json, os, time, requests\n",
        "from collections import defaultdict\n",
        "\n",
        "import regex as re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CfgNode(dict):\n",
        "    \"\"\"Simple config dictionary allowing dot notation.\"\"\"\n",
        "    def __getattr__(self, name): return self[name]\n",
        "    def __setattr__(self, name, value): self[name] = value\n",
        "    def merge_from_dict(self, d): self.update(d)"
      ],
      "metadata": {
        "id": "rXE4Tj3iRGNw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bytes_to_unicode():\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2 ** 8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2 ** 8 + n)\n",
        "            n += 1\n",
        "    return dict(zip(bs, map(chr, cs)))\n",
        "\n",
        "def get_pairs(word):\n",
        "    return set(zip(word, word[1:]))\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self, encoder, bpe_merges):\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        self.encoder = encoder\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
        "        self.cache = {}\n",
        "        self.pat = re.compile(r\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\")\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache: return self.cache[token]\n",
        "        word = tuple(token)\n",
        "        pairs = get_pairs(word)\n",
        "        if not pairs: return token\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks: break\n",
        "            first, second = bigram\n",
        "            new_word, i = [], 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "                if i < len(word) - 1 and word[i + 1] == second:\n",
        "                    new_word.append(first + second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            word = tuple(new_word)\n",
        "            if len(word) == 1: break\n",
        "            pairs = get_pairs(word)\n",
        "\n",
        "        word = ' '.join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_idx = []\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token_bytes = token.encode('utf-8')\n",
        "            token_translated = ''.join(self.byte_encoder[b] for b in token_bytes)\n",
        "            token_merged = self.bpe(token_translated).split(' ')\n",
        "            token_ix = [self.encoder[bpe_token] for bpe_token in token_merged]\n",
        "            bpe_idx.extend(token_ix)\n",
        "        return bpe_idx\n",
        "\n",
        "    def decode(self, bpe_idx):\n",
        "        tokens_merged = [self.decoder[token] for token in bpe_idx]\n",
        "        tokens_flat = ''.join(tokens_merged)\n",
        "        tokens_bytes = bytearray([self.byte_decoder[c] for c in tokens_flat])\n",
        "        return tokens_bytes.decode('utf-8', errors='replace')\n",
        "\n",
        "\n",
        "def get_encoder():\n",
        "    cache_dir = os.path.join(os.path.expanduser('~'), '.cache', 'mingpt')\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    def get_file(local, remote):\n",
        "        if not os.path.isfile(local):\n",
        "            with open(local, 'wb') as f:\n",
        "                f.write(requests.get(remote).content)\n",
        "\n",
        "    enc_path = os.path.join(cache_dir, 'encoder.json')\n",
        "    get_file(enc_path, 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json')\n",
        "    with open(enc_path, 'r') as f: encoder = json.load(f)\n",
        "\n",
        "    vocab_path = os.path.join(cache_dir, 'vocab.bpe')\n",
        "    get_file(vocab_path, 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe')\n",
        "    with open(vocab_path, 'r', encoding='utf-8') as f:\n",
        "        bpe_merges = [tuple(merge_str.split()) for merge_str in f.read().split('\\n')[1:-1]]\n",
        "\n",
        "    return Encoder(encoder, bpe_merges)"
      ],
      "metadata": {
        "id": "1viUwtD8RLGw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NewGELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * x.pow(3))))\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                             .view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.resid_dropout(self.c_proj(y))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            NewGELU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.resid_pdrop),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.block_size = config.block_size\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop=nn.Dropout(config.embd_pdrop),\n",
        "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f=nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith(\"c_proj.weight\"):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "        x = self.transformer.drop(self.transformer.wte(idx) + self.transformer.wpe(pos))\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.block_size:] if idx.size(1) > self.block_size else idx\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) if do_sample else torch.topk(probs, 1, dim=-1)[1]\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "4ZhAdVrQRPnG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharDataset(Dataset):\n",
        "    def __init__(self, text, tokenizer, block_size):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.block_size = block_size\n",
        "        self.data = tokenizer.encode(text)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        chunk = self.data[idx : idx + self.block_size + 1]\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "7cQDoYIcRRt5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataset, config):\n",
        "    model.train()\n",
        "    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        total_loss = 0.0\n",
        "        for step, (x, y) in enumerate(dataloader):\n",
        "            x, y = x.to(config.device), y.to(config.device)\n",
        "            logits, loss = model(x, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            if step % 100 == 0:\n",
        "                print(f\"Epoch {epoch} Step {step}: loss = {loss.item():.4f}\")\n",
        "        print(f\"Epoch {epoch} completed with average loss: {total_loss / len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "KcQ74Y36SO9p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    tokenizer = get_encoder()\n",
        "\n",
        "    # Download Tiny Shakespeare\n",
        "    data_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "    data_path = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"mingpt\", \"shakespeare.txt\")\n",
        "    if not os.path.isfile(data_path):\n",
        "        print(\"Downloading Tiny Shakespeare dataset...\")\n",
        "        r = requests.get(data_url)\n",
        "        with open(data_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(r.text)\n",
        "\n",
        "    with open(data_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    config = CfgNode()\n",
        "    config.model_type = 'gpt-mini'\n",
        "    config.vocab_size = 50257\n",
        "    config.block_size = 128\n",
        "    config.n_layer = 6\n",
        "    config.n_head = 6\n",
        "    config.n_embd = 192\n",
        "    config.embd_pdrop = 0.1\n",
        "    config.resid_pdrop = 0.1\n",
        "    config.attn_pdrop = 0.1\n",
        "    config.batch_size = 32\n",
        "    config.learning_rate = 3e-4\n",
        "    config.epochs = 1\n",
        "    config.grad_norm_clip = 1.0\n",
        "    config.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    model = GPT(config).to(config.device)\n",
        "    dataset = CharDataset(text, tokenizer, config.block_size)\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    train(model, dataset, config)\n",
        "\n",
        "    prompt = \"Once upon a time\"\n",
        "    input_ids = tokenizer.encode(prompt)\n",
        "    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(config.device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(input_tensor, max_new_tokens=50, do_sample=True, temperature=0.9)\n",
        "        generated = tokenizer.decode(output[0].tolist())\n",
        "\n",
        "    print(\"--- PROMPT ---\")\n",
        "    print(prompt)\n",
        "    print(\"--- GENERATED ---\")\n",
        "    print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28cFdU8zUEIa",
        "outputId": "b6d0d422-5e5d-4a52-9cd3-c7ed0e12bae3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Tiny Shakespeare dataset...\n",
            "Starting training...\n",
            "Epoch 0 Step 0: loss = 10.8547\n",
            "Epoch 0 Step 100: loss = 5.8631\n",
            "Epoch 0 Step 200: loss = 5.1259\n",
            "Epoch 0 Step 300: loss = 4.7753\n",
            "Epoch 0 Step 400: loss = 4.5102\n",
            "Epoch 0 Step 500: loss = 4.3270\n",
            "Epoch 0 Step 600: loss = 4.0366\n",
            "Epoch 0 Step 700: loss = 4.0473\n",
            "Epoch 0 Step 800: loss = 3.8545\n",
            "Epoch 0 Step 900: loss = 3.9681\n",
            "Epoch 0 Step 1000: loss = 3.6981\n",
            "Epoch 0 Step 1100: loss = 3.7519\n",
            "Epoch 0 Step 1200: loss = 3.5719\n",
            "Epoch 0 Step 1300: loss = 3.6516\n",
            "Epoch 0 Step 1400: loss = 3.5567\n",
            "Epoch 0 Step 1500: loss = 3.3861\n",
            "Epoch 0 Step 1600: loss = 3.3048\n",
            "Epoch 0 Step 1700: loss = 3.2328\n",
            "Epoch 0 Step 1800: loss = 3.2342\n",
            "Epoch 0 Step 1900: loss = 3.1277\n",
            "Epoch 0 Step 2000: loss = 3.2755\n",
            "Epoch 0 Step 2100: loss = 3.0583\n",
            "Epoch 0 Step 2200: loss = 3.0136\n",
            "Epoch 0 Step 2300: loss = 2.8334\n",
            "Epoch 0 Step 2400: loss = 3.0729\n",
            "Epoch 0 Step 2500: loss = 2.8441\n",
            "Epoch 0 Step 2600: loss = 2.8261\n",
            "Epoch 0 Step 2700: loss = 2.7673\n",
            "Epoch 0 Step 2800: loss = 2.6662\n",
            "Epoch 0 Step 2900: loss = 2.7538\n",
            "Epoch 0 Step 3000: loss = 2.6001\n",
            "Epoch 0 Step 3100: loss = 2.5692\n",
            "Epoch 0 Step 3200: loss = 2.5112\n",
            "Epoch 0 Step 3300: loss = 2.5189\n",
            "Epoch 0 Step 3400: loss = 2.4134\n",
            "Epoch 0 Step 3500: loss = 2.4524\n",
            "Epoch 0 Step 3600: loss = 2.2400\n",
            "Epoch 0 Step 3700: loss = 2.2503\n",
            "Epoch 0 Step 3800: loss = 2.1528\n",
            "Epoch 0 Step 3900: loss = 2.1618\n",
            "Epoch 0 Step 4000: loss = 2.2323\n",
            "Epoch 0 Step 4100: loss = 2.0811\n",
            "Epoch 0 Step 4200: loss = 2.0453\n",
            "Epoch 0 Step 4300: loss = 2.0504\n",
            "Epoch 0 Step 4400: loss = 2.0375\n",
            "Epoch 0 Step 4500: loss = 1.9864\n",
            "Epoch 0 Step 4600: loss = 1.9554\n",
            "Epoch 0 Step 4700: loss = 1.9178\n",
            "Epoch 0 Step 4800: loss = 1.9767\n",
            "Epoch 0 Step 4900: loss = 1.8909\n",
            "Epoch 0 Step 5000: loss = 1.7806\n",
            "Epoch 0 Step 5100: loss = 1.7498\n",
            "Epoch 0 Step 5200: loss = 1.7648\n",
            "Epoch 0 Step 5300: loss = 1.6816\n",
            "Epoch 0 Step 5400: loss = 1.7490\n",
            "Epoch 0 Step 5500: loss = 1.6972\n",
            "Epoch 0 Step 5600: loss = 1.6912\n",
            "Epoch 0 Step 5700: loss = 1.6818\n",
            "Epoch 0 Step 5800: loss = 1.6247\n",
            "Epoch 0 Step 5900: loss = 1.4930\n",
            "Epoch 0 Step 6000: loss = 1.4840\n",
            "Epoch 0 Step 6100: loss = 1.4216\n",
            "Epoch 0 Step 6200: loss = 1.4084\n",
            "Epoch 0 Step 6300: loss = 1.4381\n",
            "Epoch 0 Step 6400: loss = 1.4334\n",
            "Epoch 0 Step 6500: loss = 1.3865\n",
            "Epoch 0 Step 6600: loss = 1.4293\n",
            "Epoch 0 Step 6700: loss = 1.3957\n",
            "Epoch 0 Step 6800: loss = 1.2718\n",
            "Epoch 0 Step 6900: loss = 1.2654\n",
            "Epoch 0 Step 7000: loss = 1.3350\n",
            "Epoch 0 Step 7100: loss = 1.3497\n",
            "Epoch 0 Step 7200: loss = 1.3374\n",
            "Epoch 0 Step 7300: loss = 1.2374\n",
            "Epoch 0 Step 7400: loss = 1.2003\n",
            "Epoch 0 Step 7500: loss = 1.2409\n",
            "Epoch 0 Step 7600: loss = 1.2189\n",
            "Epoch 0 Step 7700: loss = 1.1897\n",
            "Epoch 0 Step 7800: loss = 1.1645\n",
            "Epoch 0 Step 7900: loss = 1.1523\n",
            "Epoch 0 Step 8000: loss = 1.2216\n",
            "Epoch 0 Step 8100: loss = 1.1093\n",
            "Epoch 0 Step 8200: loss = 1.2053\n",
            "Epoch 0 Step 8300: loss = 1.0630\n",
            "Epoch 0 Step 8400: loss = 1.0304\n",
            "Epoch 0 Step 8500: loss = 1.0732\n",
            "Epoch 0 Step 8600: loss = 1.1244\n",
            "Epoch 0 Step 8700: loss = 1.0659\n",
            "Epoch 0 Step 8800: loss = 1.0723\n",
            "Epoch 0 Step 8900: loss = 1.0650\n",
            "Epoch 0 Step 9000: loss = 0.9861\n",
            "Epoch 0 Step 9100: loss = 1.0184\n",
            "Epoch 0 Step 9200: loss = 0.9776\n",
            "Epoch 0 Step 9300: loss = 0.9502\n",
            "Epoch 0 Step 9400: loss = 0.9280\n",
            "Epoch 0 Step 9500: loss = 0.9959\n",
            "Epoch 0 Step 9600: loss = 0.9418\n",
            "Epoch 0 Step 9700: loss = 0.9836\n",
            "Epoch 0 Step 9800: loss = 0.8449\n",
            "Epoch 0 Step 9900: loss = 0.9720\n",
            "Epoch 0 Step 10000: loss = 0.9270\n",
            "Epoch 0 Step 10100: loss = 0.8786\n",
            "Epoch 0 Step 10200: loss = 0.9104\n",
            "Epoch 0 Step 10300: loss = 0.9583\n",
            "Epoch 0 Step 10400: loss = 0.9757\n",
            "Epoch 0 Step 10500: loss = 0.8752\n",
            "Epoch 0 completed with average loss: 2.1032\n",
            "--- PROMPT ---\n",
            "Once upon a time\n",
            "--- GENERATED ---\n",
            "Once upon a time's time, you must inquire\n",
            "Which you are to call upon, with a present\n",
            "To give out and your conference, for the noble offer lasts.\n",
            "O miserable woman, so much is a kind soul!\n",
            "O, she knew well as\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yuc6XmprUH_p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}